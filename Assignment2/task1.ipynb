{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74681dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from pyspark import SparkContext\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7dd8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "case = int(sys.argv[1])\n",
    "support = sys.argv[2]\n",
    "input_file_path = sys.argv[3]\n",
    "output_file_path = sys.argv[4]\n",
    "'''\n",
    "case = 1\n",
    "threshold = 10\n",
    "input_file_path = \"/Users/gopi/Desktop/Assignment2/small2.csv\"\n",
    "output_file_path = \"/Users/gopi/Desktop/Assignment2/output1.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb70ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3340451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(partition, total_length):\n",
    "    baskets = list(partition)\n",
    "    #print(baskets)\n",
    "    unique_items = set()\n",
    "    for basket in baskets:\n",
    "        for item in basket:\n",
    "            unique_items.add(item)\n",
    "    single_item_sets = list(unique_items)#list(set(i for sub in baskets for i in sub))\n",
    "    scaled_threshold = (len(baskets) / total_length) * threshold\n",
    "    #print(scaled_threshold)\n",
    "    candidates = list(combinations(single_item_sets,1))\n",
    "    support = get_support_candidates(candidates, baskets)\n",
    "    pruned_candidates = prune_candidates(support, scaled_threshold)\n",
    "    #print(pruned_candidates)\n",
    "    \n",
    "    result_set = []\n",
    "    result_set.append(pruned_candidates)\n",
    "    count = 2\n",
    "    while(len(pruned_candidates) > 0) :\n",
    "        item_sets = combinations(pruned_candidates, 2)\n",
    "        item_sets = reformat_item_sets(item_sets, count)\n",
    "        #print(item_sets)\n",
    "        support = get_support_candidates(item_sets, baskets)\n",
    "        pruned_candidates = prune_candidates(support, scaled_threshold)\n",
    "        if len(pruned_candidates) > 0:\n",
    "            result_set.append(pruned_candidates)\n",
    "        count+=1\n",
    "    #print(result_set)\n",
    "    return result_set   \n",
    "    \n",
    "def reformat_item_sets(item_sets, size):\n",
    "    result_dict = {}\n",
    "    for item in item_sets:\n",
    "        new_tuple = tuple(set(item[0]).union(set(item[1])))\n",
    "        if(size == len(new_tuple)):\n",
    "            new_tuple = tuple(sorted(new_tuple))\n",
    "            if new_tuple not in result_dict:\n",
    "                result_dict[new_tuple] = None\n",
    "    return list(result_dict.keys())\n",
    "\n",
    "def get_single_item_sets(baskets) :\n",
    "    result = set()\n",
    "    for basket in baskets:\n",
    "        for item in basket:\n",
    "            result.add(item)\n",
    "    return result\n",
    "\n",
    "def get_support_candidates(candidates, baskets) :\n",
    "    support = {}\n",
    "    for candidate in candidates:\n",
    "        candidate_set = set(candidate)\n",
    "        #print(candidate_set)\n",
    "        for basket in baskets:\n",
    "            if candidate_set.issubset(set(basket)):\n",
    "                support[candidate] = support[candidate] + 1 if candidate in support else 1\n",
    "    #print(support)\n",
    "    return support\n",
    "\n",
    "def prune_candidates(items, support):\n",
    "    filtered_keys = list(dict(filter(lambda item : item[1] >= support, items.items())).keys())\n",
    "    filtered_keys = sorted(filtered_keys)\n",
    "    return filtered_keys\n",
    "\n",
    "def son(partition, candidates):\n",
    "    result = {}\n",
    "    baskets = list(partition)\n",
    "    result = get_support_candidates(candidates, baskets)\n",
    "    return list(result.items())\n",
    "\n",
    "def write_data_to_file(file_path, file_mode, item_sets, data_type):\n",
    "    file=open(file_path,file_mode);\n",
    "    file.write(data_type);\n",
    "    file.write(\"\\n\")\n",
    "    \n",
    "    result = defaultdict(lambda : [])\n",
    "    \n",
    "    for item in item_sets:\n",
    "        length = len(item)\n",
    "        if(length == 1):\n",
    "            result[length].append(\"(\" + item[0] + \")\")\n",
    "        else:\n",
    "            result[length].append(str(item))\n",
    "    keys = sorted(list(result.keys()))\n",
    "    \n",
    "    for key in keys:\n",
    "        string = \",\".join(sorted(result[key]))\n",
    "        file.write(str(string))\n",
    "        file.write(\"\\n\\n\")\n",
    "    \n",
    "    file.close()\n",
    "        \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59855b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    }
   ],
   "source": [
    "initial_rdd = sc.textFile(input_file_path)\n",
    "first = initial_rdd.first()\n",
    "initial_rdd = initial_rdd.filter(lambda x : x != first).map(lambda line: line.split(\",\"))\n",
    "if case == 2:\n",
    "    initial_rdd = initial_rdd.map(lambda x : [x[1], x[0]])\n",
    "\n",
    "rdd = initial_rdd.groupByKey().map(lambda x : list(set(x[1])))\n",
    "num_partitions = rdd.getNumPartitions()\n",
    "total_length = rdd.count()\n",
    "candidate_items = rdd.mapPartitions(lambda partition : apriori(partition, total_length)).flatMap(lambda result : result).distinct().collect()\n",
    "write_data_to_file(output_file_path, \"w\", candidate_items, \"Candidates:\")\n",
    "#print(candidate_items)\n",
    "\n",
    "son = rdd.mapPartitions(lambda partition : son(partition, candidate_items)).reduceByKey(lambda a,b : a+b).filter(lambda frequent_item : frequent_item[1] >= threshold).map(lambda frequent_item : frequent_item[0]).collect()\n",
    "#son = rdd.mapPartitions(lambda partition : son(partition, candidate_items)).reduceByKey(lambda a,b : a+b).collect()\n",
    "write_data_to_file(output_file_path, \"a\", son, \"Frequent Itemsets:\")\n",
    "#print(son)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
