{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfbab644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from itertools import combinations\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc8b8284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/21 18:13:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/03/21 18:13:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/03/21 18:13:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "input_file_path = sys.argv[1]\n",
    "output_file_path = sys.argv[2]\n",
    "'''\n",
    "input_folder = \"/Users/gopi/Desktop/Assignment3/\"\n",
    "test_file_path = \"/Users/gopi/Desktop/Assignment3/yelp_val_in.csv\"\n",
    "output_file_path = \"/Users/gopi/Desktop/Assignment3/output_2_3.csv\"\n",
    "output_file_path_pairs = \"/Users/gopi/Desktop/Assignment3/output_1_sim_m.csv\"\n",
    "#SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"assign-3-task-1\").set(\"spark.executor.memory\", \"4g\").set(\"spark.driver.memory\", \"4g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed057146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pairs(user_ratings_1, user_ratings_2):\n",
    "    if user_ratings_1 is None or user_ratings_2 is None:\n",
    "        return False\n",
    "    #print(\"set 1 is :\", user_ratings_1)\n",
    "    #print(\"set 2 is :\", user_ratings_2)\n",
    "    set_1 = set(user_ratings_1.keys())\n",
    "    set_2 = set(user_ratings_2.keys())\n",
    "    intersection = list(set_1.intersection(set_2))\n",
    "    #print(\"inter is :\", intersection)\n",
    "    #print(len(intersection) > 0)\n",
    "    return len(intersection) >= 1\n",
    "\n",
    "def compute_similarity(user_ratings_1, user_ratings_2):\n",
    "    set_1 = set(user_ratings_1.keys())\n",
    "    set_2 = set(user_ratings_2.keys())\n",
    "    corated = list(set_1.intersection(set_2))\n",
    "    values_set_1 = list(user_ratings_1.values())\n",
    "    values_set_2 = list(user_ratings_2.values())\n",
    "    \n",
    "    list_1 = []\n",
    "    list_2 = []\n",
    "    for user_id in corated:\n",
    "        list_1.append(user_ratings_1[user_id])\n",
    "        list_2.append(user_ratings_2[user_id])\n",
    "    \n",
    "    avg_1 = sum(list_1) / float(len(list_1))\n",
    "    avg_2 = sum(list_2) / float(len(list_2))\n",
    "    \n",
    "    avg_1 = sum(values_set_1) / len(values_set_1)\n",
    "    avg_2 = sum(values_set_2) / len(values_set_2)   \n",
    "    \n",
    "    num = 0\n",
    "    denom_1 = 0\n",
    "    denom_2 = 0\n",
    "    for i in range(len(list_1)):\n",
    "        num += ((list_1[i] - avg_1) * (list_2[i] - avg_2))\n",
    "        denom_1 += ((list_1[i] - avg_1) ** 2)\n",
    "        denom_2 += ((list_2[i] - avg_2) ** 2)\n",
    "    \n",
    "    denom = math.sqrt(denom_1) * math.sqrt(denom_2)\n",
    "    if num == 0 or denom == 0:\n",
    "        return 0\n",
    "    \n",
    "    return num / denom\n",
    "    \n",
    "    \n",
    "def predict_rating(x):\n",
    "    if x[0] not in user_index_map:\n",
    "        return avg_business_rating[x[1]] if x[1] in business_index_map else 4.0\n",
    "    \n",
    "    if x[1] not in business_index_map:\n",
    "        return avg_user_rating[x[0]] if x[0] in user_index_map else 4.0\n",
    "    \n",
    "    target_user_id = user_index_map[x[0]]\n",
    "    target_business_id = business_index_map[x[1]]\n",
    "    user_rated_business = user_business_rating_map[target_user_id]\n",
    "    #limit = 5\n",
    "    final = []\n",
    "    for business in user_rated_business:\n",
    "        business_id = business[0]\n",
    "        key = (target_business_id, business_id) if target_business_id < business_id else (business_id, target_business_id)\n",
    "        similarity = similarity_map[key] if key in similarity_map else 0.1\n",
    "        #similarity = (similarity ** 2.5)\n",
    "        if business_id != target_business_id:\n",
    "            final.append((similarity, business[1]))\n",
    "    \n",
    "    #final = sorted(final, key = lambda x : x[0], reverse=True)\n",
    "    #final = final[:limit]\n",
    "    #print(final)\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    for pair in final:\n",
    "        num += (pair[1] * pair[0])\n",
    "        denom += pair[0]\n",
    "    if denom == 0:\n",
    "        return avg_user_rating[x[0]] if x[0] in user_index_map else 4.0\n",
    "    return num / denom\n",
    "\n",
    "def get_hybrid_prediction(pred_1, pred_2, X_index):\n",
    "    final_pred = []\n",
    "    for i in range(len(pred_1)):\n",
    "        data = [pred_1[i][0],pred_1[i][1]]\n",
    "        count = n_count_rdd[X_index[i][0]]\n",
    "        \n",
    "        if count >= 1000 :\n",
    "            new_pred = (0.9 * pred_1[i][2]) + (0.1 * pred_2[i])\n",
    "        else:\n",
    "            new_pred = (0.1 * pred_1[i][2]) + (0.9 * pred_2[i])\n",
    "        \n",
    "        data.append(new_pred)\n",
    "        final_pred.append(data)       \n",
    "    return final_pred\n",
    "\n",
    "def compute_rmse(pred_data,data_type):\n",
    "\n",
    "    file_2 = open(\"/Users/gopi/Desktop/Assignment3/yelp_val.csv\", \"r\")\n",
    "    csv_2 = csv.reader(file_2, delimiter=\",\")\n",
    "    pred_2 = []\n",
    "    c = 0\n",
    "    for row in csv_2:\n",
    "        if c != 0:\n",
    "            pred_2.append(float(row[2]))\n",
    "        c+=1\n",
    "            \n",
    "    pred_1 = []\n",
    "    if data_type == \"CF\":\n",
    "        pred_1 = [data[2] for data in pred_data]\n",
    "    else:\n",
    "        pred_1 = pred_data\n",
    "        \n",
    "    total = 0\n",
    "    for i in range(len(pred_1)):\n",
    "        total += ((pred_1[i] - pred_2[i]) ** 2)\n",
    "    \n",
    "    return math.sqrt(total/len(pred_1))\n",
    "\n",
    "def update_predictions(predictions):\n",
    "    results = []\n",
    "    for pred in predictions:\n",
    "        rating = pred if pred <= 5.0 else 5.0\n",
    "        rating = 3.0 if rating <= 1.5 else rating\n",
    "        results.append(rating)\n",
    "    return results\n",
    "\n",
    "def get_neighbours_count(x):\n",
    "    if x[0] not in user_index_map:\n",
    "        return (x[0], 0)\n",
    "    \n",
    "    if x[1] not in business_index_map:\n",
    "        return (x[0],0)\n",
    "    \n",
    "    target_user_id = user_index_map[x[0]]\n",
    "    target_business_id = business_index_map[x[1]]\n",
    "    user_rated_business = user_business_rating_map[target_user_id]\n",
    "    #limit = 5\n",
    "    final = []\n",
    "    for business in user_rated_business:\n",
    "        business_id = business[0]\n",
    "        key = (target_business_id, business_id) if target_business_id < business_id else (business_id, target_business_id)\n",
    "        similarity = similarity_map[key] if key in similarity_map else 0.1\n",
    "        #similarity = (similarity ** 2.5)\n",
    "        if business_id != target_business_id:\n",
    "            final.append((similarity, business[1]))     \n",
    "    return  (x[0], len(final))\n",
    "\n",
    "def write_data_to_file(output_file_path, data):\n",
    "    file = open(output_file_path, 'w')\n",
    "    file.write(\"user_id, business_id, prediction\")\n",
    "    file.write(\"\\n\")\n",
    "    for i in range(len(data)):\n",
    "        key = data[i]\n",
    "        rating = key[2] if key[2] < 5.0 else 5.0\n",
    "        rating = 3.0 if rating < 1.5 else rating\n",
    "        rating = round(rating, 3)\n",
    "        output = key[0] + \",\" + key[1] + \",\" + str(rating)\n",
    "        file.write(output)\n",
    "        file.write(\"\\n\")\n",
    "    file.close()   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac9ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_users(user_id) :\n",
    "    return user_id in user_id_set\n",
    "\n",
    "def filter_business(business_id):\n",
    "    return business_id in business_id_set\n",
    "\n",
    "def load_user_data(user):\n",
    "    result = [user['user_id'], user['review_count'], user['average_stars'], user.get('fans',0), user.get('useful', 0),user.get('cool',0),user.get('funny',0)]\n",
    "    return tuple(result)\n",
    "\n",
    "def load_business_data(business):\n",
    "    result = [business['business_id'], business['review_count'], business['stars'], business.get('latitude', 0), business.get('longitude',0), business_photo_count.get(business['business_id'],0)]\n",
    "    return tuple(result)\n",
    "\n",
    "def get_final_data(data):\n",
    "    time_1 = time.time()\n",
    "    user_id = data[0]\n",
    "    business_id = data[1]\n",
    "    \n",
    "    final_data = []\n",
    "    user_id_data = user_data[user_id] if user_id in user_id_set else [0 for i in range(6)]\n",
    "    business_id_data = business_data[business_id] if business_id in business_id_set else [0 for i in range(5)]\n",
    "    \n",
    "    final_data.extend(user_id_data)\n",
    "    final_data.extend(business_id_data)\n",
    "    if len(data) == 3:\n",
    "        final_data.append(data[2])\n",
    "    return final_data\n",
    "\n",
    "def update_predictions(predictions):\n",
    "    results = []\n",
    "    for pred in predictions:\n",
    "        rating = pred if pred <= 5.0 else 5.0\n",
    "        rating = 3.0 if rating <= 1.5 else rating\n",
    "        results.append(round(rating, 3))\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f742fc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "initial_rdd = sc.textFile(input_folder + \"yelp_train.csv\")\n",
    "first = initial_rdd.first()\n",
    "initial_rdd = initial_rdd.filter(lambda line : line != first).map(lambda line : line.split(\",\")).filter(lambda x : x[0] != None and x[1] != None and x[2] != None).map(lambda line: (line[0], line[1], float(line[2])))\n",
    "\n",
    "user_index_map = initial_rdd.map(lambda x : x[0]).distinct().sortBy(lambda x: x).zipWithIndex().map(lambda x : (x[0],x[1])).collectAsMap()\n",
    "business_index_map = initial_rdd.map(lambda x : x[1]).distinct().sortBy(lambda x: x).zipWithIndex().map(lambda x : (x[0],x[1])).collectAsMap()\n",
    "\n",
    "index_user_map = initial_rdd.map(lambda x : x[0]).distinct().sortBy(lambda x: x).zipWithIndex().map(lambda x : (x[1],x[0])).collectAsMap() \n",
    "index_business_map = initial_rdd.map(lambda x : x[1]).distinct().sortBy(lambda x: x).zipWithIndex().map(lambda x : (x[1],x[0])).collectAsMap()\n",
    "\n",
    "test_data_rdd = sc.textFile(test_file_path)\n",
    "first = test_data_rdd.first()\n",
    "test_data_rdd = test_data_rdd.filter(lambda x : x != first).map(lambda line : line.split(\",\"))\n",
    "business_id_set = set(test_data_rdd.map(lambda x : x[1]).collect())\n",
    "\n",
    "filtered_input = initial_rdd.map(lambda x: (business_index_map[x[1]], (user_index_map[x[0]], x[2]))).groupByKey().mapValues(list).mapValues(lambda x : dict(x))\n",
    "business_user_rating_map = filtered_input.collectAsMap()\n",
    "\n",
    "user_business_rating_map = initial_rdd.map(lambda x: (user_index_map[x[0]], (business_index_map[x[1]], x[2]))).groupByKey().mapValues(list).collectAsMap()\n",
    "\n",
    "business_items = filtered_input.map(lambda x: x[0])\n",
    "\n",
    "business_pairs = business_items.cartesian(business_items).filter(lambda x : x[0] < x[1]).filter(lambda x : x[0] in business_id_set).filter(lambda x : filter_pairs(business_user_rating_map[x[0]], business_user_rating_map[x[1]])).partitionBy(15)\n",
    "similarity_map = business_pairs.map(lambda x : (x, compute_similarity(business_user_rating_map[x[0]], business_user_rating_map[x[1]]))).filter(lambda x : x[1] > 0.01).collectAsMap()\n",
    "\n",
    "\n",
    "avg_business_rating = initial_rdd.map(lambda x : (x[1], float(x[2]))).groupByKey().mapValues(list).map(lambda x : (x[0], sum(x[1])/len(x[1]))).collectAsMap()\n",
    "avg_user_rating = initial_rdd.map(lambda x : (x[0], float(x[2]))).groupByKey().mapValues(list).map(lambda x : (x[0], sum(x[1])/len(x[1]))).collectAsMap()\n",
    "results_rdd = test_data_rdd.map(lambda x : (x[0], x[1], predict_rating(x))).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69bf86b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11270\n",
      "24732\n",
      "Time taken to load X,Y is: 79.6703188419342\n"
     ]
    }
   ],
   "source": [
    "#Model based predictions\n",
    "\n",
    "user_id_set = initial_rdd.map(lambda x : (x[0])).distinct().collect()\n",
    "business_id_set = initial_rdd.map(lambda x : (x[1])).distinct().collect()\n",
    "\n",
    "business_photo_count = sc.textFile(\"photo.json\").map(lambda x : json.loads(x)).map(lambda x : (x['business_id'], x['photo_id'], x['caption'])).filter(lambda x : len(x[2]) != 0).map(lambda x : (x[0], x[1])).groupByKey().mapValues(set).map(lambda x : (x[0], len(x[1]))).collectAsMap()\n",
    "#review_length_avg = sc.textFile(\"review_train.json\").map(lambda x : json.loads(x)).map(lambda x : (x['user_id'], len(x['text']))).groupByKey().mapValues(list).map(lambda x : (x[0], sum(x[1]) / len(x[1]))).collectAsMap()\n",
    "\n",
    "user_data = sc.textFile(\"user.json\").map(lambda x : json.loads(x)).map(lambda user : load_user_data(user)).filter(lambda x : filter_users(x[0])).map(lambda x : (x[0],(x[1:]))).collectAsMap()\n",
    "business_data = sc.textFile(\"business.json\").map(lambda x : json.loads(x)).map(lambda user : load_business_data(user)).filter(lambda x : filter_business(x[0])).map(lambda x : (x[0],(x[1:]))).collectAsMap()\n",
    "\n",
    "start_1 = time.time()\n",
    "X_rdd = initial_rdd.map(lambda x : get_final_data(x))\n",
    "#print(X_rdd.take(5))\n",
    "X = X_rdd.map(lambda x : x[:-1]).collect()\n",
    "Y = X_rdd.map(lambda x : x[-1]).collect()\n",
    "\n",
    "print(len(user_data))\n",
    "print(len(business_data))\n",
    "'''\n",
    "for key,value in user_business_rating_map.items():\n",
    "    X.append(get_final_data(key))\n",
    "    Y.append(value)\n",
    "'''\n",
    "    \n",
    "print(\"Time taken to load X,Y is:\", time.time() - start_1)\n",
    "xgboost_model = xgb.XGBRegressor()\n",
    "xgboost_model.fit(X,Y)\n",
    "\n",
    "testing_rdd = sc.textFile(test_file_path)\n",
    "testing_header = testing_rdd.first()\n",
    "testing_data_map = testing_rdd.filter(lambda x : x!= testing_header).map(lambda line : line.split(\",\")).map(lambda line: ((line[0], line[1]),\"\")).collectAsMap()\n",
    "\n",
    "X_test = []\n",
    "X_index = []\n",
    "for key in list(testing_data_map.keys()):\n",
    "    X_test.append(get_final_data(key))\n",
    "    X_index.append(key)\n",
    "\n",
    "predictions = xgboost_model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6681dc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#predictions = update_predictions(predictions)\n",
    "n_count_rdd = test_data_rdd.map(lambda x : get_neighbours_count(x)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40ce7015",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = get_hybrid_prediction(results_rdd, predictions, X_index)\n",
    "write_data_to_file(output_file_path, final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d74e807b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0754449514566038\n",
      "0.9876144246533534\n",
      "0.9875920144448801\n"
     ]
    }
   ],
   "source": [
    "print(compute_rmse(results_rdd, \"CF\"))\n",
    "print(compute_rmse(predictions, \"model\"))\n",
    "print(compute_rmse(final_pred, \"CF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9eb361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
