{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac1dd782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from itertools import combinations\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baeff760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/20 18:28:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/03/20 18:28:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "input_file_path = sys.argv[1]\n",
    "output_file_path = sys.argv[2]\n",
    "'''\n",
    "input_file_path = \"/Users/gopi/Desktop/Assignment3/yelp_train.csv\"\n",
    "test_file_path = \"/Users/gopi/Desktop/Assignment3/yelp_val_in.csv\"\n",
    "output_file_path = \"/Users/gopi/Desktop/Assignment3/output_1.csv\"\n",
    "output_file_path_pairs = \"/Users/gopi/Desktop/Assignment3/output_1_sim_m.csv\"\n",
    "#SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"assign-3-task-1\").set(\"spark.executor.memory\", \"4g\").set(\"spark.driver.memory\", \"4g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "784d8b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pairs(user_ratings_1, user_ratings_2):\n",
    "    if user_ratings_1 is None or user_ratings_2 is None:\n",
    "        return False\n",
    "    #print(\"set 1 is :\", user_ratings_1)\n",
    "    #print(\"set 2 is :\", user_ratings_2)\n",
    "    set_1 = set(user_ratings_1.keys())\n",
    "    set_2 = set(user_ratings_2.keys())\n",
    "    intersection = list(set_1.intersection(set_2))\n",
    "    #print(\"inter is :\", intersection)\n",
    "    #print(len(intersection) > 0)\n",
    "    return len(intersection) >= 1\n",
    "\n",
    "def compute_similarity(user_ratings_1, user_ratings_2):\n",
    "    set_1 = set(user_ratings_1.keys())\n",
    "    set_2 = set(user_ratings_2.keys())\n",
    "    corated = list(set_1.intersection(set_2))\n",
    "    values_set_1 = list(user_ratings_1.values())\n",
    "    values_set_2 = list(user_ratings_2.values())\n",
    "    \n",
    "    list_1 = []\n",
    "    list_2 = []\n",
    "    for user_id in corated:\n",
    "        list_1.append(user_ratings_1[user_id])\n",
    "        list_2.append(user_ratings_2[user_id])\n",
    "    \n",
    "    avg_1 = sum(list_1) / float(len(list_1))\n",
    "    avg_2 = sum(list_2) / float(len(list_2))\n",
    "    \n",
    "    avg_1 = sum(values_set_1) / len(values_set_1)\n",
    "    avg_2 = sum(values_set_2) / len(values_set_2)   \n",
    "    \n",
    "    num = 0\n",
    "    denom_1 = 0\n",
    "    denom_2 = 0\n",
    "    for i in range(len(list_1)):\n",
    "        num += ((list_1[i] - avg_1) * (list_2[i] - avg_2))\n",
    "        denom_1 += ((list_1[i] - avg_1) ** 2)\n",
    "        denom_2 += ((list_2[i] - avg_2) ** 2)\n",
    "    \n",
    "    denom = math.sqrt(denom_1) * math.sqrt(denom_2)\n",
    "    if num == 0 or denom == 0:\n",
    "        return 0\n",
    "    \n",
    "    return num / denom\n",
    "    \n",
    "    \n",
    "def predict_rating(x):\n",
    "    if x[0] not in user_index_map:\n",
    "        return avg_business_rating[x[1]] if x[1] in business_index_map else 4.0\n",
    "    \n",
    "    if x[1] not in business_index_map:\n",
    "        return avg_user_rating[x[0]] if x[0] in user_index_map else 4.0\n",
    "    \n",
    "    target_user_id = user_index_map[x[0]]\n",
    "    target_business_id = business_index_map[x[1]]\n",
    "    user_rated_business = user_business_rating_map[target_user_id]\n",
    "    #limit = 5\n",
    "    final = []\n",
    "    for business in user_rated_business:\n",
    "        business_id = business[0]\n",
    "        key = (target_business_id, business_id) if target_business_id < business_id else (business_id, target_business_id)\n",
    "        similarity = similarity_map[key] if key in similarity_map else 0.1\n",
    "        #similarity = (similarity ** 2.5)\n",
    "        if business_id != target_business_id:\n",
    "            final.append((similarity, business[1]))\n",
    "    \n",
    "    #final = sorted(final, key = lambda x : x[0], reverse=True)\n",
    "    #final = final[:limit]\n",
    "    #print(final)\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    for pair in final:\n",
    "        num += (pair[1] * pair[0])\n",
    "        denom += pair[0]\n",
    "    if denom == 0:\n",
    "        return avg_user_rating[x[0]] if x[0] in user_index_map else 4.0\n",
    "    return num / denom\n",
    "\n",
    "def write_data_to_file(output_file_path, data):\n",
    "    file = open(output_file_path, 'w')\n",
    "    file.write(\"user_id, business_id, prediction\")\n",
    "    file.write(\"\\n\")\n",
    "    for i in range(len(data)):\n",
    "        key = data[i]\n",
    "        rating = key[2]\n",
    "        output = key[0] + \",\" + key[1] + \",\" + str(rating)\n",
    "        file.write(output)\n",
    "        file.write(\"\\n\")\n",
    "    file.close()   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec4dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "initial_rdd = sc.textFile(input_file_path)\n",
    "first = initial_rdd.first()\n",
    "initial_rdd = initial_rdd.filter(lambda line : line != first).map(lambda line : line.split(\",\")).filter(lambda x : x[0] != None and x[1] != None and x[2] != None).map(lambda line: (line[0], line[1], float(line[2])))\n",
    "\n",
    "user_index_map = initial_rdd.map(lambda x : x[0]).distinct().sortBy(lambda x: x).zipWithIndex().map(lambda x : (x[0],x[1])).collectAsMap()\n",
    "business_index_map = initial_rdd.map(lambda x : x[1]).distinct().sortBy(lambda x: x).zipWithIndex().map(lambda x : (x[0],x[1])).collectAsMap()\n",
    "\n",
    "index_user_map = initial_rdd.map(lambda x : x[0]).distinct().sortBy(lambda x: x).zipWithIndex().map(lambda x : (x[1],x[0])).collectAsMap() \n",
    "index_business_map = initial_rdd.map(lambda x : x[1]).distinct().sortBy(lambda x: x).zipWithIndex().map(lambda x : (x[1],x[0])).collectAsMap()\n",
    "\n",
    "test_data_rdd = sc.textFile(test_file_path)\n",
    "first = test_data_rdd.first()\n",
    "test_data_rdd = test_data_rdd.filter(lambda x : x != first).map(lambda line : line.split(\",\"))\n",
    "business_id_set = set(test_data_rdd.map(lambda x : x[1]).collect())\n",
    "\n",
    "filtered_input = initial_rdd.map(lambda x: (business_index_map[x[1]], (user_index_map[x[0]], x[2]))).groupByKey().mapValues(list).mapValues(lambda x : dict(x))\n",
    "business_user_rating_map = filtered_input.collectAsMap()\n",
    "\n",
    "user_business_rating_map = initial_rdd.map(lambda x: (user_index_map[x[0]], (business_index_map[x[1]], x[2]))).groupByKey().mapValues(list).collectAsMap()\n",
    "\n",
    "business_items = filtered_input.map(lambda x: x[0])\n",
    "\n",
    "business_pairs = business_items.cartesian(business_items).filter(lambda x : x[0] < x[1]).filter(lambda x : x[0] in business_id_set).filter(lambda x : filter_pairs(business_user_rating_map[x[0]], business_user_rating_map[x[1]])).partitionBy(15)\n",
    "similarity_map = business_pairs.map(lambda x : (x, compute_similarity(business_user_rating_map[x[0]], business_user_rating_map[x[1]]))).filter(lambda x : x[1] > 0.01).collectAsMap()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee3daee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avg_business_rating = initial_rdd.map(lambda x : (x[1], float(x[2]))).groupByKey().mapValues(list).map(lambda x : (x[0], sum(x[1])/len(x[1]))).collectAsMap()\n",
    "avg_user_rating = initial_rdd.map(lambda x : (x[0], float(x[2]))).groupByKey().mapValues(list).map(lambda x : (x[0], sum(x[1])/len(x[1]))).collectAsMap()\n",
    "\n",
    "results_rdd = test_data_rdd.map(lambda x : (x[0], x[1], predict_rating(x))).collect()\n",
    "write_data_to_file(output_file_path, results_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2983f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0754449514566038\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def compute_rmse():\n",
    "\n",
    "    file_2 = open(\"/Users/gopi/Desktop/Assignment3/yelp_val.csv\", \"r\")\n",
    "    csv_2 = csv.reader(file_2, delimiter=\",\")\n",
    "    \n",
    "    pred_1 = []\n",
    "    pred_2 = []\n",
    "    c = 0\n",
    "    with open(\"/Users/gopi/Desktop/Assignment3/output_1.csv\") as file:\n",
    "        csv_1 = csv.reader(file, delimiter=\",\")\n",
    "        for row in csv_1:\n",
    "            if c != 0:\n",
    "                pred_1.append(float(row[2]))\n",
    "                \n",
    "            c+=1\n",
    "        c=0        \n",
    "    for row in csv_2:\n",
    "        if c != 0:\n",
    "            pred_2.append(float(row[2]))\n",
    "        c+=1\n",
    "            \n",
    "    total = 0\n",
    "    for i in range(len(pred_1)):\n",
    "        total += ((pred_1[i] - pred_2[i]) ** 2)\n",
    "    \n",
    "    return math.sqrt(total/len(pred_1))\n",
    "\n",
    "print(compute_rmse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6051735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def get_hybrid_prediction(pred_1, pred_2):\n",
    "    for i in range(len(pred_1)):\n",
    "        final_pred[i].append((0.9 * pred_2[i]) + (0.1 * pred_1[i]))\n",
    "write_data_to_file(\"/Users/gopi/Desktop/Assignment3/output_2_3.csv\", final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26c2124f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def get_neighbours_count(x):\n",
    "    if x[0] not in user_index_map:\n",
    "        return (x[0], 0)\n",
    "    \n",
    "    if x[1] not in business_index_map:\n",
    "        return (x[0],0)\n",
    "    \n",
    "    target_user_id = user_index_map[x[0]]\n",
    "    target_business_id = business_index_map[x[1]]\n",
    "    user_rated_business = user_business_rating_map[target_user_id]\n",
    "    #limit = 5\n",
    "    final = []\n",
    "    for business in user_rated_business:\n",
    "        business_id = business[0]\n",
    "        key = (target_business_id, business_id) if target_business_id < business_id else (business_id, target_business_id)\n",
    "        similarity = similarity_map[key] if key in similarity_map else 0.1\n",
    "        #similarity = (similarity ** 2.5)\n",
    "        if business_id != target_business_id:\n",
    "            final.append((similarity, business[1]))     \n",
    "    return  (x[0], len(final))\n",
    "\n",
    "n_count_rdd = test_data_rdd.map(lambda x : get_neighbours_count(x)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "538d31f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 171, 172, 174, 175, 177, 178, 179, 180, 181, 182, 183, 185, 186, 187, 189, 190, 191, 192, 193, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 208, 209, 210, 214, 216, 217, 220, 221, 222, 223, 226, 227, 230, 231, 233, 234, 235, 237, 238, 239, 240, 246, 249, 250, 252, 254, 256, 258, 261, 262, 264, 265, 267, 269, 271, 273, 274, 275, 280, 282, 285, 286, 292, 293, 294, 297, 298, 301, 304, 306, 309, 313, 325, 328, 329, 331, 348, 358, 359, 369, 374, 375, 402, 408, 418, 460, 480, 496, 572, 813, 1032, 1062]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "n_count_rdd = sorted(n_count_rdd, key = lambda x : x[1], reverse=False)\n",
    "a = []\n",
    "for value in n_count_rdd:\n",
    "    a.append(value[1])\n",
    "print(sorted(list(set(a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2406ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
